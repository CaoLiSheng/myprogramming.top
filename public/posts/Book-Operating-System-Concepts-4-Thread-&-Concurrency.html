<!DOCTYPE html><html lang="cn-zh"><head><meta charset="UTF-8" /><meta http-equiv="pragma" content="no-cache" /><meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><title>读书笔记之《Operating System Concepts》4 | 又心真人的博客</title><script> var _hmt = _hmt || []; (function () { var hm = document.createElement('script'); hm.src = 'https://hm.baidu.com/hm.js?f402a68d651d46513a3688a8d07eb93c'; var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(hm, s); })(); </script><link rel="stylesheet" type="text/css" href="antique.9570fe7e6d.css" /></head><body theme="Light" class="snapshot"><div id="main"><article class="markdown-body hidden"><h1>读书笔记之《Operating System Concepts》4</h1><h6 class="date-tag"><code> ~-~> 2021-04-15</code></h6><blockquote><p>线程和并发</p></blockquote><h2 id="motivation-vs-benefits">Motivation vs. Benefits</h2><ul><li><code>Motivation</code> – 简单三个字：被逼的。单线程是扛不住业务需求。</li><li><code>Benefits</code><ul><li><code>Responsiveness.</code> – Multithreading an interactive application may allow a program to continue running even if part of it is blocked or is performing a lengthy (time-consuming) operation, thereby increasing responsiveness to the user.</li><li><code>Resource sharing.</code> – Processes can share resources only through techniques such as shared memory and message passing. Such techniques must be explicilty arranged by the programmer. However, threads share the memory and the resources of the process to which they belong by default. The benefits of sharing code and data is that it allows an application to have several different threads of activity within the same address space.</li><li><code>Economy.</code> – It is move economical to create and context-switch threads than processes.</li><li><code>Scalability.</code> – The benefits fo multithreading can be even greater in a multiprocessor architecture, where threads may be running in parallel on different processing cores. A single-threaded process can run on only one processor, regardless how many are available.</li></ul></li></ul><h2 id="concurrency-vs-parallelism-bing-fa-vs-bing-xing">Concurrency vs. Parallelism（并发 vs. 并行）</h2><p>A concurrent system supports more than one task by allowing all the tasks to make progress. In contrast, a parallel system can perform more than one task simultaneously. Thus, it is possible to have concurrency without parallelism. Before the advent of multiprocessor and multicore architectures, most computer systems had only a single processor, and CPU schedulers were designed to provide the illusion of parallelism by rapidly switching between processes, thereby allowing each process to make progress. Such processes were running concurrently, but not in parallel.</p><h2 id="five-areas-present-challenges-in-programming-for-multicore-systems">Five Areas Present Challenges in Programming for Multicore Systems</h2><ol><li><code>Identifying tasks.</code> – This involves examining applications to find areas that can be devided into seperate, concurrent tasks. Ideally, tasks are independent of one another and thus can run in parallel on individual cores.</li><li><code>Balance.</code> – While identifying tasks that can run in parallel, programmers must also ensure that the tasks perform equal work of equal value. In some instances, a certain task may not contribute as much value to the overall process as other tasks. Using a seperate execution core to run that task may not be worth the cost.</li><li><code>Data splitting.</code> – Just as applications are divided into seperate tasks, the data accessed and manipulated by the tasks must be devided to run on seperate cores.</li><li><code>Data dependency.</code> – The data accessed by the tasks must be examined for dependencies between two or more tasks. When one task depends on data from another, programmers must ensure that the execution of the tasks is synchronized to accommodate the data dependency.</li><li><code>Testing and debugging.</code> – When a program is running in parallel on multiple cores, many different execution paths are possible. Testing and debugging such concurrent programs is inherently more difficult than testing and debugging single-threaded applications.</li></ol><h2 id="amdahls-law">AMDAHL'S LAW</h2><p>Amdahl's Law is formula that identifies potential performance gains from adding adictional computing cores to an application that has both serial (nonparallel) and parallel components. If <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span> is the portion of the application that must be performed serially on a system with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> processing cores, the formula appears as follows:</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mi>p</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>u</mi><mi>p</mi><mo>≤</mo><mfrac><mn>1</mn><mrow><mi>S</mi><mo>+</mo><mfrac><mrow><mn>1</mn><mo>−</mo><mi>S</mi></mrow><mi>N</mi></mfrac></mrow></mfrac></mrow><annotation encoding="application/x-tex">speedup \le \cfrac{1}{S + \cfrac{1 - S}{N}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">p</span><span class="mord mathnormal">ee</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≤</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.756em;vertical-align:-2.166em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5899999999999999em;"><span style="top:-2.11em;"><span class="pstrut" style="height:3.59em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5899999999999999em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.74em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span></span></span></span></span><span style="top:-3.82em;"><span class="pstrut" style="height:3.59em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-4.33em;"><span class="pstrut" style="height:3.59em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:2.166em;"><span></span></span></span></span></span><span></span></span></span></span></span></span></p><p>As an example, assume we have an application that is 75 percent parallel and 25 percent serial. If we run this application on a system with two processing cores, we can get a speedup of 1.6 times. If we add additional cores (for a total of four), the speedup is 2.28 times. Below is a graph illustrating Amdahl's Law in several different scenarios.</p><p><figure><img alt="Amdahl's Law in several different scenarios" src="../resources/Operating-System-Concepts-4-Thread-&-Concurrency/Amdahl's-Law-in-several-different-scenarios.png" title="又心真人的博客" /><figcaption>Amdahl's Law in several different scenarios</figcaption></figure></p><p>One interesting fact about Amdahl's Law is that as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> approaches infinity, the speedup converges to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mi>S</mi></mrow><annotation encoding="application/x-tex">1/S</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1/</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span></span></span></span>. For example, if 50 percent of an application is performed serially, the maximum speedup is 2.0 times, regardless of the number of processing core we add. This is the fundamental principle behind Amdahl's Law: the serial portion of an application can have a disproportionate effect on the performance we gain by addding additional computing cores.</p><h2 id="types-of-parallelism">Types of Parallelism</h2><ul><li><code>Data parallelism</code> – focuses on distributing subsets of the same data across multiple computing cores and performing the same operation on each core.</li><li><code>Task parallelism</code> – involves distributing not data but tasks (threads) across multiple computing cores. Each thread is performing aa unique operation. Different threads may be operating on the same data, or they may be operating on different data.</li></ul><p>However, data and task parallelism are not mutually exclusive, and an application may in fact use a hybrid of these two strategies.</p><h2 id="user-threads-vs-kernel-threads">User Threads vs. Kernel Threads</h2><p>However, support for threads may be provided either at the user level, for <code>user threads</code>, or by the kernel, for <code>kernel threads</code>. User threads are supported <strong>above</strong> the kernel and are managed <strong>without</strong> kernel support, whereas kernel threads are supported and managed directly by the operating system. Virtualy all contemporary operating systems – including Windows, Linux, and macOS – support kernel threads.</p><h2 id="multithreading-models">Multithreading Models</h2><ul><li><code>Many-to-One Model</code> – The many-to-one model maps many user-level threads to one kernel thread. Thread management is done by the thread library in user space, so it is efficient. However, the entire process will block if a thread makes a blocking system call. Also, because only one thread can access the kernel at a time, multiple threads are unable to run in parallel on multicore systems. For instance: <ul><li><code>Green threads</code> – a thread library available for Solaris systems and adopted in early versions of Java.</li></ul></li><li><code>One-to-One Model</code> – The one-to-one model maps each user thread to a kernel thread. It provides more concurrency than the many-to-one model by allowing another thread to run when a thread makes a blocking system call. It also allows multiple threads to run in parallel on multiprocessors. The only drawback to this model is that creating a user thread requires creating the corresponding kernel thread, and a large number of kernel threads may burden the performance of a system. <ul><li>With an increasing number of processing cores appearing on most systems, limiting the number of kernel threads has become less important. As a result, most operating systems now use the one-to-one model.</li></ul></li><li><code>Many-to-Many Model</code> – The many-to-many model multiplexes many user-level threads to a smaller or equal number of kernel threads. The number of kernel threads may be specific to either a particular application or a particular machine (for instance, an application may be allocated more kernel threads on a system with 8 processing cores than a system with 4 cores). <ul><li>Although the many-to-many model appears to be the most flexible of the models discussed, in practice it is difficult to implement. However, in some contemporary concurrency libraries have <code>developers identify tasks</code> that are then mapped to threads using the many-to-many model.</li><li>One variation one the many-to-many model still multiplexes many user-level threads to a smaller or equal number of kernel threads but also allows a user-level thread to be bound to only one kernel thread. This variation is sometimes referred to as the <code>two-level model</code>.</li></ul></li></ul><p>Consider the effect of this design on concurrency. Whereas the many-to-one model allows the developer to create as many user threads as she wishes, it does not result in parallelism, because the kernel can schedule only one kernel thread at a time. The one-to-one model allows greater concurrency, but the developer has to be careful not to create too many threads within an application. (In fact, on some systems, she still may be limited in the number of threads she can create.) The many-to-many model suffers from neither of these shortcomings: developers can create as many user threads as necessary, and the corresponding kernel threads can run in parallel on a multiprocessor. Also, when a thread performs a blocking system call, the kernel can schedule another thread for execution.</p><h2 id="asynchronous-threading-vs-synchronous-threading">Asynchronous Threading vs. Synchronous Threading</h2><ul><li><code>asynchronous threading</code> – With asynchronous threading, once the parent creates a child thread, the parent resumes its execution, so that the parent and child execute concurrently and independently of one another. Because the threads are independent, there is typically little data sharing between them. Asynchronous threading is the strategy used in <code>the multithreaded server</code> and is also commonly used for <code>designing responsive user interfaces</code>.</li><li><code>synchronous threading</code> – Synchronous threading occurs when the parent thread creates one or more children and then must wait for all of its children to terminate before it resumes. Here, the threads created by the parent perform work concurrently, but the parent cannot continue until this work has been completed. Once each thread has finished its work, it terminates and joins with its parent. Only after all of the children have joined can the parent resume execution. Typically, synchronous threading involves <code>significant data sharing among threads</code>. For example, the parent thread may combine the results calculated by its various children.</li></ul><h2 id="thread-libraries">Thread Libraries</h2><blockquote><h2 id="posix-pthreads">POSIX Pthreads</h2><p><code>Pthreads</code> refers to the <code>POSIX standard</code> (IEEE 1003.1c) defining an API for thread creation and synchronization. This is a <em><strong>specification</strong></em> for the thread behavior, <strong>not</strong> an <em><strong>implementation</strong></em>. Operating-system designers may implement the specification in any way they wish. Numerous systems implement the Pthreads specification; most are UNIX-type systems, including Linux and macOS. Although Windows doesn't support Pthreads natively, some thrid-party implementations for Windows are available. This library may be provided as either a user-level or kernel-level library.</p></blockquote><pre><code class="language-c">#include &lt;pthread.h&gt;
#include &lt;stdio.h&gt;

#include &lt;stdlib.h&gt;

int sum; /* this data is shared by the thread(s) */
void *runner(void *param); /* threads call this function */

int main(int argc, char *argv[])
{
  pthread_t tid; /* the thread identifier */
  pthread_attr_t attr; /* set of thread attributes */

  /* set the default attributes of the thread */
  pthread_attr_init(&amp;attr);
  /* create the thread */
  pthread_create(&amp;tid, &amp;attr, runner, argv[1]);
  /* wait for the thread to exit */
  pthread_join(tid, NULL);

  printf(&quot;sum = %d\n&quot;, sum);
}

/* The thread will execute in this function */
void *runner(void *param)
{
  int i, upper = atoi(param);
  sum = 0;

  for (i = 1; i &lt;= upper; i++)
  {
    sum += i;
  }

  pthread_exit(0);
}
</code></pre><blockquote><p>This example program creates only a single thread. The example program below joins on ten threads using the Pthread code.</p></blockquote><pre><code class="language-c">#define NUM_THREADS 10

/* an array of threads to be joined upon */
pthread_t workers[NUM_THREADS];

for (int i = 0; i &lt; NUM_THREADS; i++)
{
  pthread_join(workers[i], NULL);
}
</code></pre><blockquote><h2 id="windows-threads">Windows Threads</h2><p>The technique for creating threads using the Windows thread library is similar to the Pthreads technique in several ways. This library is a kernel-level library on Windows systems.</p></blockquote><pre><code class="language-c">#include &lt;windows.h&gt;
#include &lt;stdio.h&gt;

DWORD Sum; /* data is shared by the thread(s) */

/* The thread will execute in this function */
DWORD WINAPI Summation(LPVOID Param)
{
  DWORD Upper = *(DWORD*)Param;
  for (DWORD i = 1; i &lt;= Upper; i++)
  {
    Sum += i;
  }
  return 0;
}

int main(int argc, char *argv[])
{
  DWORD ThreadId;
  HANDLE ThreadHandle;
  int Param;

  Param = atoi(argv[1]);
  /* create the thread */
  ThreadHandle = CreateHandle(
    NULL, /* default security attributes */
    0, /* default stack size */
    Summation, /* thread function */
    &amp;Param, /* parameter to thread function */
    0, /* default creation flags */
    &amp;ThreadId); /* returns the thread identifier */

  /* now wait for the thread to finish */
  WaitForSingleObject(ThreadHandle, INFINITE);

  /* close the thread handle */
  CloseHandle(ThreadHandle);

  printf(&quot;sum = %d\n&quot;, Sum);
}
</code></pre><blockquote><p>The Pthread program has the parent thread wait for the summation thread using the <code>pthread_join()</code> statement. The equivalent of this in the Windows API is the <code>WaitForSingleObject()</code> function, which causes the creating thread to block until the summation thread has exited. In situations that requires waiting for multiple threads to complete, the <code>WaitForMultipleObjects()</code> funtion is used.</p></blockquote><pre><code class="language-c">WaitForMultipleOjbects(
  N, /* the number of objects to wait for */
  THandles, /* a pointer to the array of objects */
  TRUE, /* a flag indicating whether all objects have been signaled */
  INFINITE); /* a timeout duration (or INFINITE) */
</code></pre><blockquote><h2 id="java-threads">Java Threads</h2><p>All Java programs comprise at least a single thread of control – even a simple Java program consisting of only a <code>main()</code> method runs as a single thread in the JVM. Java threads are available on any system that provides a JVM including Windows, Linux, and macOS. The Java thread API is available for Android applications as well.</p><p>There are two techniques for explicitly creating threads in a Java program. One approach is to create a new class that is derived from the Thread class and to override its <code>run()</code> method. An alternative – and more commonly used – technique is to define a class that implements the <code>Runnable</code> interface. This interface defines a single abstract method with the signature <code>public void run()</code>. The code in the <code>run()</code> method of a class that implements <code>Runnable</code> is what executes in a separate thread. An example is shown below:</p></blockquote><pre><code class="language-java">class Task implements Runnable {
  public void run() {
    System.out.println(&quot;I'm a thread.&quot;);
  }
}

/* thread creation somewhere */
Thread worker = new Thread(new Task());

/* Lambda expressions in Java */
Runnable task = () -&gt; {
    System.out.println(&quot;I'm a thread.&quot;);
};
Thread worker = new Thread(task);

/* allocates memory and initializes a new thread in the JVM */
/* implicitly calls the run() method, making the thread eligible to be run by the JVM */
worker.start();

/* wait for the thread to finish */
try {
  worker.join();
} catch (InterruptedException e) { }
</code></pre><blockquote><h3 id="java-executor-framework">Java Executor Framework</h3><p>Java has supported thread creation using the approach above far since its origins. However, beginning with Version 1.5 and its API, Java introduced several new concurrency features that provide developers with much greater control over thread creation and communication. These tools are available in the <code>java.util.concurrent</code> package.</p></blockquote><pre><code class="language-java">public interface Executor {
  void execute(Runnable command);
}

Executor service = new Executor...;
service.execute(new Task());
</code></pre><pre><code class="language-java">import java.util.concurrent.*;

class Summation implements Callable&lt;Integer&gt; {
  private int upper;

  public Summation(int upper) {
    this.upper = upper;
  }

  /* The thread will execute in this method */
  public Integer call() {
    int sum = 0;
    for (int i = 1; i &lt;= upper; i++) {
      sum += i;
    }
    return new Integer(sum);
  }
}

public class Driver {
  public static void main(String[] args) {
    int upper = Integer.parseInt(args[0]);

    ExecutorService pool = Executors.newSingleThreadExecutor();
    Future&lt;Integer&gt; result = pool.submit(new Summation(upper));

    try {
      System.out.println(&quot;sum = &quot; + result.get());
    } catch (InterruptedException | ExecutionException e) { }
  }
}
</code></pre><h2 id="implicit-threading">Implicit Threading</h2><p>With the continued growth of multicore processing, applications containing hundreds – or even thousands – of threads are looming on the horizon. Designing such applications is not a trivial undertaking: programmers must address additional challenges which relate to program correctness – <code>Synchronization</code> and <code>Deadlocks</code>.</p><p>One way to address these challenges and better support the design of concurrent and parallel applications is to transfer the creation and management of threading from application developers to <code>compilers</code> and <code>run-time libraries</code>. This strategy, termed <strong>implicit threading</strong>, is an increasingly popular trend. Implicit threading generally require application developers to identify <em><strong>tasks</strong></em> – not threads – that can run in parallel. A task is usually writen as a function, which the run-time libraries then maps to a seperate thread, typically using the <strong>many-to-many model</strong>. The advantage of this approach is that developers only need to identify parallel tasks, and the libraries determine the specific details of thread creation and management.</p><blockquote><h2 id="thread-pools">Thread Pools</h2><p>Thread pools offer these benefits:</p><ol><li>Servicing a request with an existing thread is often faster than waiting to create a thread.</li><li>A thread pool limits the number of threads that exist at any one point. This is particularly important on systems that cannot support a large number of concurrent threads.</li><li>Separating the task to be performed from the mechanics of creating the task allows us to use different strategies for running the task. For example, the task could be scheduled to execute after a time delay or to execute periodically.</li></ol><p>The number of threads in the pool can be set heuristically based on factors such as the number of CPUs in the system, the amount of physical memory, and the expected number of concurrent client requests. More sophisticated thread-pool architectures can dynamically adjust the number of threads in the pool according to usage patterns. For instance, Apple's Grand Central Dispatch is one such architecture.</p></blockquote><pre><code class="language-c">DWORD WINAPI PoolFunction(PVOID Param)
{
  /* this function runs as a seperate thread. */
}

QueueUserWorkItem(
  &amp;PoolFunction, /* a pointer to the function that is to run as a seperate thread */
  NULL, /* the parameter passed to Function */
  0); /* flags indicating how the thread pool is to create and manage execution of the thread */

/**
 * Other members in the Windows thread pool API include utilities that invoke functions
 * at periodic intervals or when an asynchronous I/O request completes.
 */
</code></pre><h2 id="another-copy-of-summary-in-the-book">Another COPY of Summary in the Book</h2><h2 id="bi-ji-mu-lu">笔记目录</h2><ol><li><a href="scroll-to-the-very-top">回到开头</a></li></ol><br /><br /><hr /><br /><div class="comments"><a href="mailto:954382491@qq.com?subject=评价「读书笔记之《Operating System Concepts》4」">来聊两句吧～</a><h6 class="tip">⚠️ 请先安装一款邮件软件（部分浏览器可能不支持，请使用设备默认浏览器打开本页面）</h6></div><br /></article></div><script type="application/javascript" src="template.5028980877.js"></script></body></html>